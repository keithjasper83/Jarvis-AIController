# Model configuration
runner_mode: lmstudio  # lmstudio | llama_cpp | ollama
lmstudio_base_url: http://host.docker.internal:1234
small_model: Qwen2.5-Coder-1.5B-Instruct-GGUF/Qwen2.5-Coder-1.5B-instruct-q8_0.guff
medium_model: QQwen2.5-Coder-7B-Instruct-GGUF/wen2.5-Coder-7B-instruct-q4_k_m.guff
large_model: Qwen2.5-Coder-14B-Instruct-GGUF/Qwen2.5-Coder-14B-instruct-q4_k_m.guff
embedding_model: bge-base-en
max_context_tokens:
  small: 8192
  medium: 16384
  large: 32768
